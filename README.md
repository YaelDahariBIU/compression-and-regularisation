# Neural Network Compression with Quantisation and Pruning

[![arXiv](https://img.shields.io/badge/arXiv-2001.04850-b31b1b.svg)](https://arxiv.org/abs/2001.04850)

This repository reproduces and extends the work from the paper  
**"Quantisation and Pruning for Neural Network Compression and Regularisation"**  
by Kimessha Paupamah, Steven James, and Richard Klein.

Our implementation supports:
- **Iterative pruning** based on sensitivity scans
- **Per-channel quantisation**
- Overfitting correction through sparsification
- Multiple architectures: `AlexNet`, `MobileNetV2`, `ShuffleNetV2`
- Datasets: `CIFAR-10`, `FashionMNIST`

---

## ğŸ§  Background

This project explores how **pruning** and **quantisation** can be combined to compress deep neural networks - reducing storage and compute cost, and potentially improving generalisation.

We reproduce and analyse the findings of the original paper using **PyTorch 1.8+**, correcting bugs, updating APIs, and conducting additional experiments on modern CPUs/GPUs.

Read our full report [here](./Quantisation%20and%20Pruning%20for%20Neural%20Network%20Compression%20and%20Regularisation%20-%20Paper%20Reproduction.pdf).

---

## ğŸ—ï¸ Methods

### ğŸ”ª Pruning

We use two types of vanilla pruning:
- **Element-wise pruning**: Remove individual weights based on absolute or custom importance
- **Filter-wise pruning**: Remove entire filters based on norm metrics

The `Pruner` class in [`pruner.py`](./pruner.py) supports stage-wise pruning with configurable rules via `.rule` files.

### ğŸ§® Quantisation

We apply **per-channel quantisation** using PyTorch's built-in quantisation pipeline, adapted in [`quantiser.py`](./quantiser.py), including fusion and calibration steps for:
- AlexNet
- MobileNetV2
- ShuffleNetV2

Quantisation reduces model precision to 8-bit integers (int8), accelerating inference and shrinking file size.

---

## ğŸƒ Usage

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train a Model

```bash
python run.py --train 1 --arch alexnet --data fmnist
```

### 3. Prune a Model

```bash
python run.py --prune 1 --load AlexNet.pth --sensitivity 0.5
```

### 4. Quantise a Model
```bash
python run.py --quantise 1 --load AlexNet_pruned.pth
```

### 5. Overfit the Prune
```bash
python run.py --overfit 1 --load AlexNet.pth
python run.py --prune 1 --load AlexNet_overfitted.pth
```

## ğŸ“ Project Structure
```bash
â”œâ”€â”€ run.py                   # Main entrypoint
â”œâ”€â”€ pruner.py                # Pruning logic
â”œâ”€â”€ quantiser.py             # Quantisation logic
â”œâ”€â”€ trainer.py               # Training and evaluation loop
â”œâ”€â”€ utils.py                 # Model, dataset, and rule utilities
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ rules/                   # Pruning rule files
â””â”€â”€ models/                  # Saved and quantised models
```

## ğŸ“Š Results Overview
| Architecture | Dataset      | Accuracy â†‘ | Parameters â†“ | Size â†“        | Inference â†“  |
| ------------ | ------------ | ---------- | ------------ | ------------- | ------------ |
| AlexNet      | FashionMNIST | +1.13%     | 57M â†’ 5M     | 217MB â†’ 55MB  | 11ms â†’ 5ms   |
| MobileNetV2  | CIFAR-10     | â‰ˆ0%        | 2.2M â†’ 671K  | 8.7MB â†’ 2.5MB | 35ms â†’ 4.5ms |
| ShuffleNetV2 | FashionMNIST | â‰ˆ0%        | 1.2M â†’ 815K  | 4.9MB â†’ 1.4MB | 13ms â†’ 7.5ms |

**Pruning** helps reduce overfitting and model size.
**Quantisation** provides speedups and compression with minimal accuracy loss.

## ğŸ§ª Reproducing the Paper
This repo is a reproduction of the methods proposed in
ğŸ“„ [Quantisation and Pruning for Neural Network Compression and Regularisation](https://www.arxiv.org/pdf/2001.04850)

Original code (broken): [kpaupamah/compression-and-regularisation](https://github.com/kpaupamah/compression-and-regularisation)

## âœï¸ Authors
Yael & Yuval Dahari.  
BIU Department of Computer Science.

